---
date: '2025-02-28'
permalink: https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:42a07e4...MicrosoftDocs:63a40ca
summary: この差分の主なポイントは、Azure AI Foundryに関連する新しいリンクの追加とPhi-4ファミリーモデルの情報更新です。新たにPhi-4-multimodal-instructモデルが追加され、テキスト、画像、音声の処理が可能です。また、Azure
  AI Foundryの情報がハイパーリンクとして追加され、ユーザーはリソースへのアクセスが容易になりました。文法の調整も行われ、可読性が向上しています。これにより、ユーザー体験が向上し、効率的なプロジェクトの推進が期待されます。
title: Diff Insight Report - misc

---

[View Diff on GitHub](https://github.com/MicrosoftDocs/azure-ai-docs/compare/MicrosoftDocs:42a07e4...MicrosoftDocs:63a40ca){target="_blank"}

<format>
# ハイライト
この差分の主なポイントは、Azure AI Foundryに関連する新しいリンクの追加と、Phi-4ファミリーモデルの情報更新です。これにより、ユーザーは関連情報に簡単にアクセスできるようになると同時に、最新のモデル情報を閲覧できます。

## 新機能
- Phi-4-multimodal-instructモデルが追加されました。これは、128Kトークンの軽量オープンマルチモーダル基盤モデルであり、テキスト、画像、音声の処理が可能です。

## 破壊的変更
特に破壊的な変更は見られません。

## その他の更新
- ドキュメント中のAzure AI Foundryに関する情報がハイパーリンクとして追加され、直接リソースにアクセスできるようになりました。
- 文法の調整、説明の明瞭化が複数のドキュメントに施され、情報の可読性が向上しました。

# インサイト
今回の更新では、Azure AI Foundryの情報を利用者に与えることに重点を置いています。ハイパーリンクが追加されることで、ユーザーは最小限の操作で関連リソースやドキュメントに容易にアクセスできます。これにより、情報を探す手間が省かれ、ユーザビリティが向上し、開発者が求める具体的な情報に基づいた迅速な意思決定を支援します。

また、Phi-4ファミリーモデルの追加により、Azure AIで利用可能なモデルの選択肢が拡大し、ユーザーがその利用シナリオに適したモデルをより容易に選択できるようになりました。これは研究者や開発者にとって新しい開発の可能性を広げる重要な要素となります。

全体として、情報の正確性とアクセス性を高めるための一連の更新により、Azure AIのユーザー体験が向上し、プロジェクトの効率的な推進に貢献することが期待されます。
</format>

# Summary Table
|  Filename  | Type |    Title    | Status | A  | D  | M  |
|------------|------|-------------|--------|----|----|----|
| [azure-openai-in-ai-studio.md](#item-07639b) | minor update | Azure OpenAI Studioのリンクを追加 | modified | 1 | 1 | 2 | 
| [connections.md](#item-01b26a) | minor update | 接続に関する情報の更新 | modified | 9 | 9 | 18 | 
| [content-filtering.md](#item-91b372) | minor update | コンプライアンスに関するリンクの更新 | modified | 1 | 1 | 2 | 
| [fine-tuning-overview.md](#item-31b07b) | minor update | ファインチューニングに関する説明の強化 | modified | 25 | 21 | 46 | 
| [faq.yml](#item-e7baa2) | minor update | FAQ文書の説明の更新 | modified | 1 | 1 | 2 | 
| [create-manage-compute-session.md](#item-6ed743) | minor update | Azure AI Foundryへのリンクの追加 | modified | 1 | 1 | 2 | 
| [create-projects.md](#item-cb10b3) | minor update | Azure AI Foundryへのリンクの追加 | modified | 1 | 1 | 2 | 
| [deploy-models-phi-4.md](#item-c40212) | minor update | Phi-4-multimodal-instructモデルに関する情報の追加 | modified | 63 | 15 | 78 | 
| [ai-template-get-started.md](#item-d71b59) | minor update | AIテンプレートの使用方法に関する情報の更新 | modified | 31 | 30 | 61 | 
| [create-hub-project-sdk.md](#item-8c3e99) | minor update | Azure AI Foundryリソースに関するリンクの追加 | modified | 1 | 1 | 2 | 
| [sdk-overview.md](#item-d3ab19) | minor update | SDK概要に関する情報の更新 | modified | 31 | 29 | 60 | 
| [vscode.md](#item-24bd97) | minor update | VS Code関連の情報にリンクの追加 | modified | 1 | 1 | 2 | 
| [model-catalog-overview.md](#item-278001) | minor update | Microsoft Phiファミリーモデルの表に新しいエントリーの追加 | modified | 1 | 1 | 2 | 
| [region-availability-maas.md](#item-35d79c) | minor update | Phi-4ファミリーのモデルに新しい項目の追加 | modified | 1 | 1 | 2 | 
| [get-started-code.md](#item-8a5082) | minor update | Azure AI Foundryリンクの追加 | modified | 1 | 1 | 2 | 
| [copilot-sdk-build-rag.md](#item-b77dba) | minor update | Azure AI Foundryリンクの追加 | modified | 1 | 1 | 2 | 
| [copilot-sdk-create-resources.md](#item-552960) | minor update | Azure AI Foundryリンクの追加 | modified | 1 | 1 | 2 | 
| [copilot-sdk-evaluate.md](#item-bb5754) | minor update | Azure AI Foundryリンクの追加 | modified | 1 | 1 | 2 | 
| [screen-reader.md](#item-4dc029) | minor update | Azure AI Foundryリンクの追加 | modified | 1 | 1 | 2 | 


# Modified Contents
## articles/ai-studio/azure-openai-in-ai-studio.md{#item-07639b}

<details>
<summary>Diff</summary>
````diff
@@ -16,7 +16,7 @@ ms.custom: ignite-2023, build-2024, ignite-2024
 
 # What is Azure OpenAI in Azure AI Foundry portal?
 
-Azure OpenAI Service provides REST API access to OpenAI's powerful language models. Azure OpenAI Studio was previously where you went to access and work with the Azure OpenAI Service. This studio is now integrated into Azure AI Foundry portal. 
+Azure OpenAI Service provides REST API access to OpenAI's powerful language models. Azure OpenAI Studio was previously where you went to access and work with the Azure OpenAI Service. This studio is now integrated into [Azure AI Foundry portal](https://ai.azure.com). 
 
 ## Access Azure OpenAI Service in Azure AI Foundry portal
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Azure OpenAI Studioのリンクを追加"
}
```

### Explanation
この変更では、ドキュメントの内容に対してマイナーな更新が行われました。具体的には、Azure OpenAI Studioに関する説明文にリンクが追加され、Azure AI FoundryポータルのURL（[Azure AI Foundry portal](https://ai.azure.com)）が明示されるように修正されました。これにより、ユーザーはAzure OpenAI StudioとAzure AI Foundryポータルとの関連性をより容易に理解し、必要な情報にアクセスしやすくなります。この変更は文書の明瞭性を向上させることを目的としています。

## articles/ai-studio/concepts/connections.md{#item-01b26a}

<details>
<summary>Diff</summary>
````diff
@@ -9,8 +9,8 @@ ms.custom:
   - build-2024
   - ignite-2024
 ms.topic: conceptual
-ms.date: 11/21/2024
-ms.reviewer: sgilley
+ms.date: 02/21/2025
+ms.reviewer: meerakurup
 ms.author: sgilley
 author: sdgilley
 ---
@@ -31,30 +31,30 @@ As another example, you can [create a connection](../how-to/connections-add.md)
 
 ## Connections to non-Microsoft services
 
-Azure AI Foundry supports connections to non-Microsoft services, including the following:
-- The [API key connection](../how-to/connections-add.md) handles authentication to your specified target on an individual basis. This is the most common non-Microsoft connection type.
-- The [custom connection](../how-to/connections-add.md) allows you to securely store and access keys while storing related properties, such as targets and versions. Custom connections are useful when you have many targets that or cases where you wouldn't need a credential to access. LangChain scenarios are a good example where you would use custom service connections. Custom connections don't manage authentication, so you'll have to manage authentication on your own.
+Azure AI Foundry supports connections to non-Microsoft services, including:
+- The [API key connection](../how-to/connections-add.md) handles authentication to your specified target on an individual basis. API key is the most common non-Microsoft connection type.
+- The [custom connection](../how-to/connections-add.md) allows you to securely store and access keys while storing related properties, such as targets and versions. Custom connections are useful when you have many targets that or cases where you wouldn't need a credential to access. LangChain scenarios are a good example where you would use custom service connections. Custom connections don't manage authentication, so you have to manage authentication on your own.
 
 ## Connections to datastores
 
 > [!IMPORTANT]
-> Data connections cannot be shared across projects. They are created exclusively in the context of one project. 
+> Data connections can't be shared across projects. They're created exclusively in the context of one project. 
 
 Creating a data connection allows you to access external data without copying it to your project. Instead, the connection provides a reference to the data source.
 
 A data connection offers these benefits:
 
 - A common, easy-to-use API that interacts with different storage types including Microsoft OneLake, Azure Blob, and Azure Data Lake Gen2.
 - Easier discovery of useful connections in team operations.
-- For credential-based access (service principal/SAS/key), Azure AI Foundry connection secures credential information. This way, you won't need to place that information in your scripts.
+- Credential-based access (service principal/SAS/key). Azure AI Foundry connection secures credential information so you don't need to place that information in your scripts.
 
 When you create a connection with an existing Azure storage account, you can choose between two different authentication methods:
 
 - **Credential-based**: Authenticate data access with a service principal, shared access signature (SAS) token, or account key. Users with *Reader* project permissions can access the credentials.
 - **Identity-based**: Use your Microsoft Entra ID or managed identity to authenticate data access.
 
     > [!TIP]
-    > When using an identity-based connection, Azure role-based access control (Azure RBAC) is used to determine who can access the connection. You must assign the correct Azure RBAC roles to your developers before they can use the connection. For more information, see [Scenario: Connections using Microsoft Entra ID](rbac-ai-studio.md#scenario-connections-using-microsoft-entra-id-authentication).
+    > When you use an identity-based connection, Azure role-based access control (Azure RBAC) determines who can access the connection. You must assign the correct Azure RBAC roles to your developers before they can use the connection. For more information, see [Scenario: Connections using Microsoft Entra ID](rbac-ai-studio.md#scenario-connections-using-microsoft-entra-id-authentication).
 
 
 The following table shows the supported Azure cloud-based storage services and authentication methods:
@@ -82,7 +82,7 @@ A Uniform Resource Identifier (URI) represents a storage location on your local
 
 ## Key vaults and secrets
 
-Connections allow you to securely store credentials, authenticate access, and consume data and information.  Secrets associated with connections are securely persisted in the corresponding Azure Key Vault, adhering to robust security and compliance standards. As an administrator, you can audit both shared and project-scoped connections on a hub level (link to connection rbac). 
+Connections allow you to securely store credentials, authenticate access, and consume data and information.  Secrets associated with connections are securely persisted in the corresponding Azure Key Vault, adhering to robust security and compliance standards. As an administrator, you can audit both shared and project-scoped connections on a hub level. 
 
 Azure connections serve as key vault proxies, and interactions with connections are direct interactions with an Azure key vault. Azure AI Foundry connections store API keys securely, as secrets, in a key vault. The key vault [Azure role-based access control (Azure RBAC)](./rbac-ai-studio.md) controls access to these connection resources. A connection references the credentials from the key vault storage location for further use. You won't need to directly deal with the credentials after they're stored in the hub's key vault. You have the option to store the credentials in the YAML file. A CLI command or SDK can override them. We recommend that you avoid credential storage in a YAML file, because a security breach could lead to a credential leak.  
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "接続に関する情報の更新"
}
```

### Explanation
この変更では、「connections.md」ドキュメントに対して複数のマイナーな更新が行われました。主な改訂点として、文書の更新日とレビュー担当者が変更され、内容の一部が整理されて明確化されました。また、いくつかの文が文法的に修正されています。特に、非Microsoftサービスへの接続の説明が改善され、APIキー接続とカスタム接続の定義がより明確に表現されています。さらに、データコネクションがプロジェクト間で共有できない旨の説明が強調され、認証情報の安全な管理に関する詳細が整理されました。これにより、ユーザーは接続の設定とセキュリティに関する理解が深まります。全体として、情報の正確性と文書の可読性が向上しています。

## articles/ai-studio/concepts/content-filtering.md{#item-91b372}

<details>
<summary>Diff</summary>
````diff
@@ -86,7 +86,7 @@ The configurability feature allows customers to adjust the settings, separately
 
 <sup>1</sup> For Azure OpenAI models, only customers who have been approved for modified content filtering have full content filtering control, including configuring content filters at severity level high only or turning off content filters. Apply for modified content filters via these forms: [Azure OpenAI Limited Access Review: Modified Content Filters](https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR7en2Ais5pxKtso_Pz4b1_xUMlBQNkZMR0lFRldORTdVQzQ0TEI5Q1ExOSQlQCN0PWcu), and [Modified Abuse Monitoring](https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR7en2Ais5pxKtso_Pz4b1_xUOE9MUTFMUlpBNk5IQlZWWkcyUEpWWEhGOCQlQCN0PWcu).
 
-Customers are responsible for ensuring that applications integrating Azure OpenAI comply with the [Code of Conduct](/legal/cognitive-services/openai/code-of-conduct?context=%2Fazure%2Fai-services%2Fopenai%2Fcontext%2Fcontext). 
+Customers are responsible for ensuring that applications integrating Azure OpenAI comply with the [Code of Conduct](/legal/ai-code-of-conduct?context=%2Fazure%2Fai-services%2Fopenai%2Fcontext%2Fcontext). 
 
 
 ## Next steps
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "コンプライアンスに関するリンクの更新"
}
```

### Explanation
この変更では、「content-filtering.md」ドキュメントにおいて、Azure OpenAIに関連するコンプライアンスのリンクが更新されました。具体的には、顧客がAzure OpenAIを統合したアプリケーションが遵守すべき「行動規範」へのリンクが、旧リンクから新しいリンクに変更されました。この修正により、ユーザーは最新の行動規範にアクセスしやすくなり、関連するコンプライアンスの要件がより明確に示されます。このような情報の更新は、ユーザーの理解を深め、適切な行動を促すために重要です。変更自体は比較的小規模ですが、文書の正確性を高める重要な修正です。

## articles/ai-studio/concepts/fine-tuning-overview.md{#item-31b07b}

<details>
<summary>Diff</summary>
````diff
@@ -1,24 +1,25 @@
 ---
 title: Fine-tuning in Azure AI Foundry portal
 titleSuffix: Azure AI Foundry
-description: This article introduces fine-tuning of models in Azure AI Foundry portal.
+description: This article explains what fine-tuning is and under what circumstances you should consider doing it.
 manager: scottpolly
 ms.service: azure-ai-foundry
 ms.custom:
   - build-2024
   - code01
-ms.topic: conceptual
-ms.date: 10/31/2024
-ms.reviewer: sgilley
+ms.topic: concept-article
+ms.date: 02/21/2025
+ms.reviewer: keli19
 ms.author: sgilley
 author: sdgilley
+#customer intent: As a developer, I want to learn what it means to fine-tune a model.
 ---
 
 # Fine-tune models with Azure AI Foundry
 
-[!INCLUDE [feature-preview](../includes/feature-preview.md)]
+Fine-tuning customizes a pretrained AI model with additional training on a specific task or dataset to improve performance, add new skills, or enhance accuracy. The result is a new, optimized GenAI model based on the provided examples.
 
-Fine-tuning refers to customizing a pre-trained generative AI model with additional training on a specific task or new dataset for enhanced performance, new skills, or improved accuracy. The result is a new, custom GenAI model that's optimized based on the provided examples.
+[!INCLUDE [feature-preview](../includes/feature-preview.md)]
 
 Consider fine-tuning GenAI models to:
 - Scale and adapt to specific enterprise needs
@@ -27,19 +28,20 @@ Consider fine-tuning GenAI models to:
 - Save time and resources with faster and more precise results
 - Get more relevant and context-aware outcomes as models are fine-tuned for specific use cases
 
-[Azure AI Foundry](https://ai.azure.com) offers several models across model providers enabling you to get access to the latest and greatest in the market. You can discover supported models for fine-tuning through our model catalog by using the **Fine-tuning tasks** filter and selecting the model card to learn detailed information about each model. Specific models may be subjected to regional constraints, [view this list for more details](#supported-models-for-fine-tuning). 
+[Azure AI Foundry](https://ai.azure.com) offers several models across model providers enabling you to get access to the latest and greatest in the market. You can discover supported models for fine-tuning through our model catalog by using the **Fine-tuning tasks** filter and selecting the model card to learn detailed information about each model. Specific models might be subjected to regional constraints. [View this list for more details](#supported-models-for-fine-tuning). 
 
 :::image type="content" source="../media/concepts/model-catalog-fine-tuning.png" alt-text="Screenshot of Azure AI Foundry model catalog and filtering by Fine-tuning tasks." lightbox="../media/concepts/model-catalog-fine-tuning.png":::
 
-This article will walk you through use-cases for fine-tuning and how this can help you in your GenAI journey.
+This article walks you through use-cases for fine-tuning and how it helps you in your GenAI journey.
 
 ## Getting started with fine-tuning
 
 When starting out on your generative AI journey, we recommend you begin with prompt engineering and RAG to familiarize yourself with base models and its capabilities. 
 - [Prompt engineering](../../ai-services/openai/concepts/prompt-engineering.md) is a technique that involves designing prompts using tone and style details, example responses, and intent mapping for natural language processing models. This process improves accuracy and relevancy in responses, to optimize the performance of the model.
 - [Retrieval-augmented generation (RAG)](../concepts/retrieval-augmented-generation.md) improves LLM performance by retrieving data from external sources and incorporating it into a prompt. RAG can help businesses achieve customized solutions while maintaining data relevance and optimizing costs.
 
-As you get comfortable and begin building your solution, it's important to understand where prompt engineering falls short and that will help you realize if you should try fine-tuning. 
+As you get comfortable and begin building your solution, it's important to understand where prompt engineering falls short and when you should try fine-tuning.
+
 - Is the base model failing on edge cases or exceptions? 
 - Is the base model not consistently providing output in the right format?
 - Is it difficult to fit enough examples in the context window to steer the model?
@@ -53,26 +55,29 @@ _A customer wants to use GPT-3.5 Turbo to turn natural language questions into q
 
 ### Use cases
 
-Base models are already pre-trained on vast amounts of data and most times you'll add instructions and examples to the prompt to get the quality responses that you're looking for - this process is called "few-shot learning". Fine-tuning allows you to train a model with many more examples that you can tailor to meet your specific use-case, thus improving on few-shot learning. This can reduce the number of tokens in the prompt leading to potential cost savings and requests with lower latency. 
+Base models are already pretrained on vast amounts of data. Most times you add instructions and examples to the prompt to get the quality responses that you're looking for - this process is called "few-shot learning." Fine-tuning allows you to train a model with many more examples that you can tailor to meet your specific use-case, thus improving on few-shot learning. Fine-tuning can reduce the number of tokens in the prompt leading to potential cost savings and requests with lower latency. 
+
+Turning natural language into a query language is just one use case where you can  "_show not tell_" the model how to behave. Here are some other use cases:
 
-Turning natural language into a query language is just one use case where you can  _show not tell_ the model how to behave. Here are some additional use cases:
 - Improve the model's handling of retrieved data
 - Steer model to output content in a specific style, tone, or format
 - Improve the accuracy when you look up information
 - Reduce the length of your prompt
-- Teach new skills (i.e. natural language to code)
+- Teach new skills (that is, natural language to code)
 
-If you identify cost as your primary motivator, proceed with caution. Fine-tuning might reduce costs for certain use cases by shortening prompts or allowing you to use a smaller model. But there may be a higher upfront cost to training, and you have to pay for hosting your own custom model. 
+If you identify cost as your primary motivator, proceed with caution. Fine-tuning might reduce costs for certain use cases by shortening prompts or allowing you to use a smaller model. But there might be a higher upfront cost to training, and you have to pay for hosting your own custom model. 
 
 ### Steps to fine-tune a model
+
 Here are the general steps to fine-tune a model:
-1. Based on your use case, choose a model that supports your task
-2. Prepare and upload training data
-3. (Optional) Prepare and upload validation data
-4. (Optional) Configure task parameters
-5. Train your model. 
-6. Once completed, review metrics and evaluate model. If the results don't meet your benchmark, then go back to step 2.
-7. Use your fine-tuned model
+
+1. Choose a model that supports your task.
+1. Prepare and upload training data.
+1. (Optional) Prepare and upload validation data.
+1. (Optional) Configure task parameters.
+1. Train your model. 
+1. Once completed, review metrics and evaluate model. If the results don't meet your benchmark, then go back to step 2.
+1. Use your fine-tuned model.
 
 It's important to call out that fine-tuning is heavily dependent on the quality of data that you can provide. It's best practice to provide hundreds, if not thousands, of training examples to be successful and get your desired results.
 
@@ -87,7 +92,6 @@ For more information on fine-tuning using a managed compute (preview), see [Fine
 
 For details about Azure OpenAI models that are available for fine-tuning, see the [Azure OpenAI Service models documentation](../../ai-services/openai/concepts/models.md#fine-tuning-models) or the [Azure OpenAI models table](#fine-tuning-azure-openai-models) later in this guide.
 
-
 For the Azure OpenAI  Service models that you can fine tune, supported regions for fine-tuning include North Central US, Sweden Central, and more.
 
 ### Fine-tuning Azure OpenAI models
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "ファインチューニングに関する説明の強化"
}
```

### Explanation
この変更では、「fine-tuning-overview.md」ドキュメントに大幅な内容の修正が加えられ、ファインチューニングの定義や目的がより明確に記載されています。具体的には、ファインチューニングの機能やその利点についての説明が簡潔かつ明瞭になり、少しずつ文が整理されました。また、マネージャーやレビュー担当者の情報が更新され、最新の日付が反映されました。

さらに、ファインチューニングの必要性を識別するための質問リストが追加され、ユーザーがファインチューニングを考慮すべき状況が明示されました。ラインごとの細かな文や用語が整理され、全体的に読みやすさが向上しました。このような更新により、読者はファインチューニングに対する理解を深め、自らのプロジェクトにおける適用方法を考慮しやすくなります。

## articles/ai-studio/faq.yml{#item-e7baa2}

<details>
<summary>Diff</summary>
````diff
@@ -14,7 +14,7 @@ metadata:
   author: sdgilley
 title: Azure AI Foundry frequently asked questions
 summary: |
-  If you can't find answers to your questions in this document, and still need help check the [Azure AI services support options guide](../ai-services/cognitive-services-support-options.md?context=/azure/ai-services/openai/context/context). Azure OpenAI is part of Azure AI services.
+  FAQ for [Azure AI Foundry](https://ai.azure.com). If you can't find answers to your questions in this document, and still need help check the [Azure AI services support options guide](../ai-services/cognitive-services-support-options.md?context=/azure/ai-services/openai/context/context). Azure OpenAI is part of Azure AI services.
 sections:
   - name: General questions
     questions:
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "FAQ文書の説明の更新"
}
```

### Explanation
この変更では、「faq.yml」ドキュメントの冒頭に含まれる説明が改善され、より具体的な情報が追加されました。具体的には、「Azure AI Foundry」に関連するFAQが明示され、文書の目的がわかりやすくなりました。

変更前の文は一般的な問い合わせに焦点を当てていましたが、変更後は「Azure AI Foundry」を具体的に示すことで、利用者が何に関するFAQかを一目で理解できるようになっています。また、サポートオプションのガイドへのリンクは保持されており、引き続き必要な情報を探しやすい構成になっています。このような更新は、ユーザーが情報を迅速に見つける助けとなり、全体的なユーザーエクスペリエンスを向上させるものです。

## articles/ai-studio/how-to/create-manage-compute-session.md{#item-6ed743}

<details>
<summary>Diff</summary>
````diff
@@ -20,7 +20,7 @@ author: sdgilley
 
 [!INCLUDE [feature-preview](../includes/feature-preview.md)]
 
-You need a compute session to run [prompt flows](prompt-flow.md). Use Azure AI Foundry to create and manage prompt flow compute sessions.
+You need a compute session to run [prompt flows](prompt-flow.md). Use [Azure AI Foundry](https://ai.azure.com) to create and manage prompt flow compute sessions.
 
 A prompt flow compute session has computing resources that are required for the application to run, including a Docker image that contains all necessary dependency packages. In addition to flow execution, Azure AI Foundry uses the compute session to ensure the accuracy and functionality of the tools incorporated within the flow when you make updates to the prompt or code content.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Azure AI Foundryへのリンクの追加"
}
```

### Explanation
この変更では、「create-manage-compute-session.md」ドキュメント内の文が修正され、Azure AI Foundryへのリンクが新たに追加されました。具体的には、コンピュートセッションを作成および管理するための情報が強化され、ユーザーは「Azure AI Foundry」の具体的なリソースに直接アクセスできるようになりました。

リンクの追加により、ドキュメントがよりインタラクティブになり、ユーザーが必要な情報をすぐに見つけやすくなります。この変更は、ユーザーエクスペリエンスを向上させ、Azure AI Foundryの使用を促す目的があります。他の部分は変わっておらず、コンピュートセッションが必要な理由やその機能に関する説明も維持されています。

## articles/ai-studio/how-to/create-projects.md{#item-cb10b3}

<details>
<summary>Diff</summary>
````diff
@@ -18,7 +18,7 @@ author: sdgilley
 
 # Create a project in Azure AI Foundry portal
 
-This article describes how to create an Azure AI Foundry project. A project is used to organize your work and save state while building customized AI apps. 
+This article describes how to create an [Azure AI Foundry](https://ai.azure.com) project. A project is used to organize your work and save state while building customized AI apps. 
 
 Projects are hosted by an Azure AI Foundry hub. If your company has an administrative team that has created a hub for you, you can create a project from that hub. If you are working on your own, you can create a project and a default hub will automatically be created for you.
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Azure AI Foundryへのリンクの追加"
}
```

### Explanation
この変更では、「create-projects.md」ドキュメントの冒頭で、Azure AI Foundryに関する情報が更新され、具体的なリンクが追加されました。文中に「[Azure AI Foundry](https://ai.azure.com)」というリンクが追加されたことで、読者は直ちに関連情報にアクセスできるようになりました。

この修正により、ユーザーがプロジェクトの作成方法を学ぶ際に、Azure AI Foundryの公式サイトへのリファレンスが明確になり、より便利な情報提供が実現されました。他のコンテンツはそのまま維持されており、プロジェクトがどのように機能し、どのように組織されるかという説明も保持されています。全体的に、これによりドキュメントの使い勝手が向上しました。

## articles/ai-studio/how-to/deploy-models-phi-4.md{#item-c40212}

<details>
<summary>Diff</summary>
````diff
@@ -29,6 +29,18 @@ The Phi-4 family of small language models (SLMs) is a collection of instruction-
 
 The Phi-4 family chat models include the following models:
 
+# [Phi-4-multimodal-instruct](#tab/phi-4-multimodal-instruct)
+
+Phi-4-multimodal-instruct is a lightweight open multimodal foundation model that leverages the language, vision, and speech research and datasets used for Phi-3.5 and 4.0 models. The model processes text, image, and audio inputs, and generates text outputs. The model underwent an enhancement process, incorporating both supervised fine-tuning, and direct preference optimization to support precise instruction adherence and safety measures.
+
+The Phi-4-multimodal-instruct model comes in the following variant with a 128K token length.
+
+
+The following models are available:
+
+* [Phi-4-multimodal-instruct](https://aka.ms/azureai/landing/Phi-4-multimodal-instruct)
+
+
 # [Phi-4-mini-instruct](#tab/phi-4-mini-instruct)
 
 Phi-4-mini-instruct is a lightweight open model built upon synthetic data and filtered publicly available websites - with a focus on high-quality, reasoning dense data. The model belongs to the Phi-4 model family and supports 128K token context length. The model underwent an enhancement process, incorporating both supervised fine-tuning and direct preference optimization to support precise instruction adherence and robust safety measures. 
@@ -155,7 +167,7 @@ print("Model provider name:", model_info.model_provider_name)
 ```
 
 ```console
-Model name: Phi-4-mini-instruct
+Model name: Phi-4-multimodal-instruct
 Model type: chat-completions
 Model provider name: Microsoft
 ```
@@ -176,7 +188,7 @@ response = client.complete(
 ```
 
 > [!NOTE]
-> Phi-4-mini-instruct and Phi-4 don't support system messages (`role="system"`). When you use the Azure AI model inference API, system messages are translated to user messages, which is the closest capability available. This translation is offered for convenience, but it's important for you to verify that the model is following the instructions in the system message with the right level of confidence.
+> Phi-4-multimodal-instruct, Phi-4-mini-instruct, and Phi-4 don't support system messages (`role="system"`). When you use the Azure AI model inference API, system messages are translated to user messages, which is the closest capability available. This translation is offered for convenience, but it's important for you to verify that the model is following the instructions in the system message with the right level of confidence.
 
 The response is as follows, where you can see the model's usage statistics:
 
@@ -192,7 +204,7 @@ print("\tCompletion tokens:", response.usage.completion_tokens)
 
 ```console
 Response: As of now, it's estimated that there are about 7,000 languages spoken around the world. However, this number can vary as some languages become extinct and new ones develop. It's also important to note that the number of speakers can greatly vary between languages, with some having millions of speakers and others only a few hundred.
-Model: Phi-4-mini-instruct
+Model: Phi-4-multimodal-instruct
 Usage: 
   Prompt tokens: 19
   Total tokens: 91
@@ -341,6 +353,18 @@ except HttpResponseError as ex:
 
 The Phi-4 family chat models include the following models:
 
+# [Phi-4-multimodal-instruct](#tab/phi-4-multimodal-instruct)
+
+Phi-4-multimodal-instruct is a lightweight open multimodal foundation model that leverages the language, vision, and speech research and datasets used for Phi-3.5 and 4.0 models. The model processes text, image, and audio inputs, and generates text outputs. The model underwent an enhancement process, incorporating both supervised fine-tuning, and direct preference optimization to support precise instruction adherence and safety measures.
+
+The Phi-4-multimodal-instruct model comes in the following variant with a 128K token length.
+
+
+The following models are available:
+
+* [Phi-4-multimodal-instruct](https://aka.ms/azureai/landing/Phi-4-multimodal-instruct)
+
+
 # [Phi-4-mini-instruct](#tab/phi-4-mini-instruct)
 
 Phi-4-mini-instruct is a lightweight open model built upon synthetic data and filtered publicly available websites - with a focus on high-quality, reasoning dense data. The model belongs to the Phi-4 model family and supports 128K token context length. The model underwent an enhancement process, incorporating both supervised fine-tuning and direct preference optimization to support precise instruction adherence and robust safety measures. 
@@ -465,7 +489,7 @@ console.log("Model provider name: ", model_info.body.model_provider_name)
 ```
 
 ```console
-Model name: Phi-4-mini-instruct
+Model name: Phi-4-multimodal-instruct
 Model type: chat-completions
 Model provider name: Microsoft
 ```
@@ -488,7 +512,7 @@ var response = await client.path("/chat/completions").post({
 ```
 
 > [!NOTE]
-> Phi-4-mini-instruct and Phi-4 don't support system messages (`role="system"`). When you use the Azure AI model inference API, system messages are translated to user messages, which is the closest capability available. This translation is offered for convenience, but it's important for you to verify that the model is following the instructions in the system message with the right level of confidence.
+> Phi-4-multimodal-instruct, Phi-4-mini-instruct, and Phi-4 don't support system messages (`role="system"`). When you use the Azure AI model inference API, system messages are translated to user messages, which is the closest capability available. This translation is offered for convenience, but it's important for you to verify that the model is following the instructions in the system message with the right level of confidence.
 
 The response is as follows, where you can see the model's usage statistics:
 
@@ -508,7 +532,7 @@ console.log("\tCompletion tokens:", response.body.usage.completion_tokens);
 
 ```console
 Response: As of now, it's estimated that there are about 7,000 languages spoken around the world. However, this number can vary as some languages become extinct and new ones develop. It's also important to note that the number of speakers can greatly vary between languages, with some having millions of speakers and others only a few hundred.
-Model: Phi-4-mini-instruct
+Model: Phi-4-multimodal-instruct
 Usage: 
   Prompt tokens: 19
   Total tokens: 91
@@ -676,6 +700,18 @@ catch (error) {
 
 The Phi-4 family chat models include the following models:
 
+# [Phi-4-multimodal-instruct](#tab/phi-4-multimodal-instruct)
+
+Phi-4-multimodal-instruct is a lightweight open multimodal foundation model that leverages the language, vision, and speech research and datasets used for Phi-3.5 and 4.0 models. The model processes text, image, and audio inputs, and generates text outputs. The model underwent an enhancement process, incorporating both supervised fine-tuning, and direct preference optimization to support precise instruction adherence and safety measures.
+
+The Phi-4-multimodal-instruct model comes in the following variant with a 128K token length.
+
+
+The following models are available:
+
+* [Phi-4-multimodal-instruct](https://aka.ms/azureai/landing/Phi-4-multimodal-instruct)
+
+
 # [Phi-4-mini-instruct](#tab/phi-4-mini-instruct)
 
 Phi-4-mini-instruct is a lightweight open model built upon synthetic data and filtered publicly available websites - with a focus on high-quality, reasoning dense data. The model belongs to the Phi-4 model family and supports 128K token context length. The model underwent an enhancement process, incorporating both supervised fine-tuning and direct preference optimization to support precise instruction adherence and robust safety measures. 
@@ -815,7 +851,7 @@ Console.WriteLine($"Model provider name: {modelInfo.Value.ModelProviderName}");
 ```
 
 ```console
-Model name: Phi-4-mini-instruct
+Model name: Phi-4-multimodal-instruct
 Model type: chat-completions
 Model provider name: Microsoft
 ```
@@ -837,7 +873,7 @@ Response<ChatCompletions> response = client.Complete(requestOptions);
 ```
 
 > [!NOTE]
-> Phi-4-mini-instruct and Phi-4 don't support system messages (`role="system"`). When you use the Azure AI model inference API, system messages are translated to user messages, which is the closest capability available. This translation is offered for convenience, but it's important for you to verify that the model is following the instructions in the system message with the right level of confidence.
+> Phi-4-multimodal-instruct, Phi-4-mini-instruct, and Phi-4 don't support system messages (`role="system"`). When you use the Azure AI model inference API, system messages are translated to user messages, which is the closest capability available. This translation is offered for convenience, but it's important for you to verify that the model is following the instructions in the system message with the right level of confidence.
 
 The response is as follows, where you can see the model's usage statistics:
 
@@ -853,7 +889,7 @@ Console.WriteLine($"\tCompletion tokens: {response.Value.Usage.CompletionTokens}
 
 ```console
 Response: As of now, it's estimated that there are about 7,000 languages spoken around the world. However, this number can vary as some languages become extinct and new ones develop. It's also important to note that the number of speakers can greatly vary between languages, with some having millions of speakers and others only a few hundred.
-Model: Phi-4-mini-instruct
+Model: Phi-4-multimodal-instruct
 Usage: 
   Prompt tokens: 19
   Total tokens: 91
@@ -1023,6 +1059,18 @@ catch (RequestFailedException ex)
 
 The Phi-4 family chat models include the following models:
 
+# [Phi-4-multimodal-instruct](#tab/phi-4-multimodal-instruct)
+
+Phi-4-multimodal-instruct is a lightweight open multimodal foundation model that leverages the language, vision, and speech research and datasets used for Phi-3.5 and 4.0 models. The model processes text, image, and audio inputs, and generates text outputs. The model underwent an enhancement process, incorporating both supervised fine-tuning, and direct preference optimization to support precise instruction adherence and safety measures.
+
+The Phi-4-multimodal-instruct model comes in the following variant with a 128K token length.
+
+
+The following models are available:
+
+* [Phi-4-multimodal-instruct](https://aka.ms/azureai/landing/Phi-4-multimodal-instruct)
+
+
 # [Phi-4-mini-instruct](#tab/phi-4-mini-instruct)
 
 Phi-4-mini-instruct is a lightweight open model built upon synthetic data and filtered publicly available websites - with a focus on high-quality, reasoning dense data. The model belongs to the Phi-4 model family and supports 128K token context length. The model underwent an enhancement process, incorporating both supervised fine-tuning and direct preference optimization to support precise instruction adherence and robust safety measures. 
@@ -1113,7 +1161,7 @@ The response is as follows:
 
 ```json
 {
-    "model_name": "Phi-4-mini-instruct",
+    "model_name": "Phi-4-multimodal-instruct",
     "model_type": "chat-completions",
     "model_provider_name": "Microsoft"
 }
@@ -1139,7 +1187,7 @@ The following example shows how you can create a basic chat completions request
 ```
 
 > [!NOTE]
-> Phi-4-mini-instruct and Phi-4 don't support system messages (`role="system"`). When you use the Azure AI model inference API, system messages are translated to user messages, which is the closest capability available. This translation is offered for convenience, but it's important for you to verify that the model is following the instructions in the system message with the right level of confidence.
+> Phi-4-multimodal-instruct, Phi-4-mini-instruct, and Phi-4 don't support system messages (`role="system"`). When you use the Azure AI model inference API, system messages are translated to user messages, which is the closest capability available. This translation is offered for convenience, but it's important for you to verify that the model is following the instructions in the system message with the right level of confidence.
 
 The response is as follows, where you can see the model's usage statistics:
 
@@ -1149,7 +1197,7 @@ The response is as follows, where you can see the model's usage statistics:
     "id": "0a1234b5de6789f01gh2i345j6789klm",
     "object": "chat.completion",
     "created": 1718726686,
-    "model": "Phi-4-mini-instruct",
+    "model": "Phi-4-multimodal-instruct",
     "choices": [
         {
             "index": 0,
@@ -1206,7 +1254,7 @@ You can visualize how streaming generates content:
     "id": "23b54589eba14564ad8a2e6978775a39",
     "object": "chat.completion.chunk",
     "created": 1718726371,
-    "model": "Phi-4-mini-instruct",
+    "model": "Phi-4-multimodal-instruct",
     "choices": [
         {
             "index": 0,
@@ -1229,7 +1277,7 @@ The last message in the stream has `finish_reason` set, indicating the reason fo
     "id": "23b54589eba14564ad8a2e6978775a39",
     "object": "chat.completion.chunk",
     "created": 1718726371,
-    "model": "Phi-4-mini-instruct",
+    "model": "Phi-4-multimodal-instruct",
     "choices": [
         {
             "index": 0,
@@ -1280,7 +1328,7 @@ Explore other parameters that you can specify in the inference client. For a ful
     "id": "0a1234b5de6789f01gh2i345j6789klm",
     "object": "chat.completion",
     "created": 1718726686,
-    "model": "Phi-4-mini-instruct",
+    "model": "Phi-4-multimodal-instruct",
     "choices": [
         {
             "index": 0,
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Phi-4-multimodal-instructモデルに関する情報の追加"
}
```

### Explanation
この変更では、「deploy-models-phi-4.md」ドキュメントにおいて新しいモデル「Phi-4-multimodal-instruct」に関する情報が追加されました。このモデルは、テキスト、画像、音声を処理できる軽量のオープンマルチモーダル基盤モデルであり、Phi-3.5および4.0モデルで使用される研究とデータセットを活かしています。

新たに追加された内容には、特に以下の要素が含まれています：
- Phi-4-multimodal-instructモデルが、128Kトークンの長さを持つこと。
- このモデルの紹介や、どのように高精度の指示遵守と安全対策を支持するために強化されたかについての説明。
- このモデルへのアクセスを提供するリンクも文中に追加されています。

また、旧モデル「Phi-4-mini-instruct」に関連する部分にもモデル名の変更が反映されています。この更新により、ドキュメントは最新の情報を反映し、ユーザーが新しいモデルの特徴についてより知識を得られるようになりました。全体として、ドキュメントの情報が充実し、ユーザーの理解を助けるためのアップデートが行われました。

## articles/ai-studio/how-to/develop/ai-template-get-started.md{#item-d71b59}

<details>
<summary>Diff</summary>
````diff
@@ -1,58 +1,59 @@
 ---
 title: How to get started with an AI template
 titleSuffix: Azure AI Foundry
-description: This article provides instructions on how to get started with an AI template.
+description: This article provides instructions on how to use an AI template to get started with Azure AI Foundry.
 manager: scottpolly
 ms.service: azure-ai-foundry
 ms.custom:
   - ignite-2024
 ms.topic: how-to
-ms.date: 01/02/2025
-ms.reviewer: dantaylo
+ms.date: 02/20/2025
+ms.reviewer: varundua
 ms.author: sgilley
 author: sdgilley
+#customer intent: As a developer, I want to jump start my journey with an AI template.
 ---
 
-# How to get started with an AI template
+# Get started with an AI template
+
+Streamline your code-first development with prebuilt, task-specific Azure AI templates. Benefit from using the latest features and best practices from Microsoft Azure AI, with popular frameworks like LangChain, prompt flow, and Semantic Kernel in multiple languages.
 
 [!INCLUDE [feature-preview](../../includes/feature-preview.md)]
 
-Streamline your code-first development with prebuilt, task-specific Azure AI templates. Benefit from using the latest features and best practices from Microsoft Azure AI, with popular frameworks like LangChain, prompt flow, and Semantic Kernel in multiple languages.
+## Prerequisites
 
-> [!TIP]
-> Discover the latest templates in our curated [AZD templates collection](https://aka.ms/azd-ai-templates). Deploy them with a single command ```azd up``` using the [Azure Developer CLI](/azure/developer/azure-developer-cli/). 
+- [Azure subscription](https://azure.microsoft.com/free)
+- An [Azure AI Foundry project](../create-projects.md).
 
 ## Start with a sample application
 
-Start with our sample applications! Choose the right template for your needs, then refer to the README in any of the following Azure Developer CLI enabled templates for more instructions and information.
+1. Go to [Azure AI Foundry portal](https://ai.azure.com).
+1. Open your project in Azure AI Foundry portal.
+1. On the left menu, select **Code** (preview).
+1. Find the solution template you want to use.
+1. Select **Open in Github** to view the entire sample application.
+1. Or, clone the repository to your local machine with the provided command.
+1. In some cases, you can also view a step-by-step tutorial that explains the AI code.
 
-### [Python](#tab/python)
+## Explore the sample application
 
-| Template      | App host | Tech stack | Description |
-| ----------- | ----------| ----------- | ------------|
-| [Azure AI Basic Template with Python](https://github.com/azure-samples/azureai-basic-python) | [Azure AI Foundry online endpoints](/azure/machine-learning/concept-endpoints-online) | [Azure Managed Identity](/entra/identity/managed-identities-azure-resources/overview), [Azure OpenAI Service](../../../ai-services/openai/overview.md), Bicep | The app serves as a straightforward example of integrating Azure AI Services within a basic prompt-based application. This template walks you through building a simple chat app that utilizes models and prompts. The template also covers setting up the necessary infrastructure for the app, including creating an Azure AI Foundry Hub, configuring projects, and provisioning essential resources such as Azure AI Service, Azure Container Apps, Cognitive Search, and more. <br>You can build, deploy, and test it with a single command.  |
-| [Contoso Chat Retail copilot with Azure AI Foundry](https://github.com/Azure-Samples/contoso-chat) | [Azure Container Apps](/azure/container-apps/overview) | [Azure Cosmos DB](/azure/cosmos-db/index-overview), [Azure Managed Identity](/entra/identity/managed-identities-azure-resources/overview), [Azure OpenAI Service](../../../ai-services/openai/overview.md), [Azure AI Search](/azure/search/search-what-is-azure-search), Bicep  | A retailer conversation agent that can answer questions grounded in your product catalog and customer order history. This template uses a retrieval augmented generation architecture with cutting-edge models for chat completion, chat evaluation, and embeddings. Build, evaluate, and deploy, an end-to-end solution with a single command. | 
-| [Process Automation: speech to text and summarization with Azure AI Foundry](https://github.com/Azure-Samples/summarization-openai-python-prompflow) | [Azure AI Foundry online endpoints](/azure/machine-learning/concept-endpoints-online) | [Azure Managed Identity](/entra/identity/managed-identities-azure-resources/overview), [Azure OpenAI Service](../../../ai-services/openai/overview.md), [Azure AI speech to text service](../../../ai-services/speech-service/index-speech-to-text.yml), Bicep  | An app for workers to report issues via text or speech, translating audio to text, summarizing it, and specify the relevant department. | 
-| [Multi-Modal Creative Writing copilot with Dalle](https://github.com/Azure-Samples/agent-openai-python-prompty) | [Azure AI Foundry online endpoints](/azure/machine-learning/concept-endpoints-online) | [Azure AI Search](/azure/search/search-what-is-azure-search), [Azure OpenAI Service](../../../ai-services/openai/overview.md), Bicep | demonstrates how to create and work with AI agents. The app takes a topic and instruction input and then calls a research agent, writer agent, and editor agent. |  
-| [Assistant API Analytics Copilot with Python and Azure AI Foundry](https://github.com/Azure-Samples/assistant-data-openai-python-promptflow) | [Azure AI Foundry online endpoints](/azure/machine-learning/concept-endpoints-online) |  [Azure Managed Identity](/entra/identity/managed-identities-azure-resources/overview), [Azure AI Search](/azure/search/search-what-is-azure-search), [Azure OpenAI Service](../../../ai-services/openai/overview.md), Bicep| A data analytics chatbot based on the Assistants API. The chatbot can answer questions in natural language, and interpret them as queries on an example sales dataset. |
-<!-- remove for now
-| Function Calling with Prompty, LangChain, and Pinecone | [Azure AI Foundry online endpoints](/azure/machine-learning/concept-endpoints-online) | [Azure Managed Identity](/entra/identity/managed-identities-azure-resources/overview), [Azure OpenAI Service](../../../ai-services/openai/overview.md), [LangChain](https://python.langchain.com/v0.1/docs/get_started/introduction), [Pinecone](https://www.pinecone.io/), Bicep  | Utilize the new Prompty tool, LangChain, and Pinecone to build a large language model (LLM) search agent. This agent with Retrieval-Augmented Generation (RAG) technology is capable of answering user questions based on the provided data by integrating real-time information retrieval with generative responses. | 
-| Function Calling with Prompty, LangChain, and Elastic Search | [Azure AI Foundry online endpoints](/azure/machine-learning/concept-endpoints-online) | [Azure Managed Identity](/entra/identity/managed-identities-azure-resources/overview), [Azure OpenAI Service](../../../ai-services/openai/overview.md), [Elastic Search](https://www.elastic.co/elasticsearch), [LangChain](https://python.langchain.com/v0.1/docs/get_started/introduction) , Bicep  | Utilize the new Prompty tool, LangChain, and Elasticsearch to build a large language model (LLM) search agent. This agent with Retrieval-Augmented Generation (RAG) technology is capable of answering user questions based on the provided data by integrating real-time information retrieval with generative responses |
--->
+Once you're looking at the GitHub repository for your sample, refer to the README for more instructions and information on how to deploy your own version of the application.
 
+Instructions vary by sample, but most include how to:
 
-### [C#](#tab/csharp)
+* Open the solution in the location of your choice:
+  * GitHub Codespaces
+  * VS Code Dev Containers
+  * Your local IDE
+* Deploy the application to Azure
+* How to test the app
 
-| Template      | App host | Tech stack | Description |
-| ----------- | ----------| ----------- | -------------- |
-| [Contoso Chat Retail copilot with .NET and Semantic Kernel](https://github.com/Azure-Samples/contoso-chat-csharp-prompty) | [Azure Container Apps](/azure/container-apps/overview) | [Azure Cosmos DB](/azure/cosmos-db/index-overview), [Azure Monitor](/azure/azure-monitor/overview), [Azure Managed Identity](/entra/identity/managed-identities-azure-resources/overview), [Azure Container Apps](/azure/container-apps/overview), [Azure AI Search](/azure/search/search-what-is-azure-search), [Azure OpenAI Services](../../../ai-services/openai/overview.md), [Semantic Kernel](/semantic-kernel/overview/?tabs=Csharp), Bicep | A retailer conversation agent that can answer questions grounded in your product catalog and customer order history. This template uses a retrieval augmented generation architecture with cutting-edge models for chat completion, chat evaluation, and embeddings. Build, evaluate, and deploy, an end-to-end solution with a single command. |
-| [Process Automation: speech to text and summarization with .NET and GPT 3.5 Turbo](https://github.com/Azure-Samples/summarization-openai-csharp-prompty) | [Azure Container Apps](/azure/container-apps/overview) | [Azure Managed Identity](/entra/identity/managed-identities-azure-resources/overview), [Azure OpenAI Service](../../../ai-services/openai/overview.md), [Azure AI speech to text service](../../../ai-services/speech-service/index-speech-to-text.yml), Bicep | An app for workers to report issues via text or speech, translating audio to text, summarizing it, and specify the relevant department. |
-
----
+The README also includes information about the application, such as the use case, architecture, and pricing information.
 
+> [!TIP]
+> Discover more templates in our curated [AZD templates collection](https://azure.github.io/ai-app-templates). Deploy them with a single command ```azd up``` using the [Azure Developer CLI](/azure/developer/azure-developer-cli/).
 
 ## Related content
 
-- [Get started building a chat app using the prompt flow SDK](../../quickstarts/get-started-code.md)
-- [Work with projects in VS Code](vscode.md)
-- [Connections in Azure AI Foundry portal](../../concepts/connections.md)
+- [Get started building a chat app using the Azure AI Foundry SDK](../../quickstarts/get-started-code.md)
+- [Develop AI apps using Azure AI services](/azure/developer/ai/)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "AIテンプレートの使用方法に関する情報の更新"
}
```

### Explanation
この変更では、「ai-template-get-started.md」ドキュメントにおいて、AIテンプレートを使用してAzure AI Foundryを開始するための手順が更新されました。主な変更点には、説明文の更新や新しい情報の追加が含まれています。

具体的には、以下のポイントが更新されました：
- 説明文が変更され、AIテンプレートを使用することによるAzure AI Foundryの開始方法がより明確に伝えられるようになりました。
- 手順が整理され、Azure AI Foundryポータルでのサンプルアプリケーションの使用方法が詳細に記述されています。具体的には、GitHubからサンプルアプリケーションを開き、必要に応じてローカルの環境にクローンする方法が示されています。
- また、特定のサンプルアプリケーションに対する展開手順やテスト方法についても言及されています。

さらに、最新の情報やリソースへのリンクが盛り込まれ、ユーザーは簡単にアクセスできるようになっています。全体として、このドキュメントの更新により、開発者がAIテンプレートを効果的に利用するための利便性が向上しました。

## articles/ai-studio/how-to/develop/create-hub-project-sdk.md{#item-8c3e99}

<details>
<summary>Diff</summary>
````diff
@@ -16,7 +16,7 @@ author: sdgilley
 
 [!INCLUDE [feature-preview](../../includes/feature-preview.md)]
 
-In this article, you learn how to create the following Azure AI Foundry resources using the Azure Machine Learning SDK and Azure CLI (with machine learning extension):
+In this article, you learn how to create the following [Azure AI Foundry](https://ai.azure.com) resources using the Azure Machine Learning SDK and Azure CLI (with machine learning extension):
 - An Azure AI Foundry hub
 - An Azure AI Services connection
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Azure AI Foundryリソースに関するリンクの追加"
}
```

### Explanation
この変更では、「create-hub-project-sdk.md」ドキュメントにおいて、Azure AI Foundryのリソースに対するリンクが追加されました。具体的には、記事内で紹介されている「Azure AI Foundry」という用語に対して、関連するウェブサイトへのリンク（[Azure AI Foundry](https://ai.azure.com)）が加えられています。

この修正により、読者はAzure AI Foundryに関する詳しい情報に簡単にアクセスできるようになり、リソースの作成方法に関する指示がより明確になります。このような軽微な変更は、ユーザーエクスペリエンスを向上させるために重要であり、文書の情報提供の質を高める役割を果たします。全体として、リソースへのアクセスが容易になることで、開発者にとってより使いやすい内容になっています。

## articles/ai-studio/how-to/develop/sdk-overview.md{#item-d3ab19}

<details>
<summary>Diff</summary>
````diff
@@ -8,7 +8,7 @@ ms.custom:
   - build-2024
   - ignite-2024
 ms.topic: overview
-ms.date: 11/25/2024
+ms.date: 02/27/2025
 ms.reviewer: dantaylo
 ms.author: sgilley
 author: sdgilley
@@ -18,7 +18,7 @@ zone_pivot_groups: programming-languages-sdk-overview
 
 # The Azure AI Foundry SDK
 
-The Azure AI Foundry SDK is a comprehensive toolchain designed to simplify the development of AI applications on Azure. It enables developers to:
+The [Azure AI Foundry](https://ai.azure.com) SDK is a comprehensive toolchain designed to simplify the development of AI applications on Azure. It enables developers to:
 
 - Access popular models from various model providers through a single interface
 - Easily combine together models, data, and AI services to build AI-powered applications
@@ -111,7 +111,7 @@ Not yet available in C#.
 
 Copy the **Project connection string** from the **Overview** page of the project and update the connections string value above.
 
-Once you have created the project client, you can use the client for the capabilities in the following sections.
+Once you create the project client, you can use the client for the capabilities in the following sections.
 
 ::: zone pivot="programming-language-python"
 
@@ -127,7 +127,7 @@ Be sure to check out the [reference](https://aka.ms/aifoundrysdk/reference) and
 
 ## Azure OpenAI Service
 
-The [Azure OpenAI Service](../../../ai-services/openai/overview.md) provides access to OpenAI's models including the GPT-4o, GPT-4o mini, GPT-4, GPT-4 Turbo with Vision, DALLE-3, Whisper, and Embeddings model series with the data residency, scalability, safety, security and enterprise capabilities of Azure.
+The [Azure OpenAI Service](../../../ai-services/openai/overview.md) provides access to OpenAI's models including the GPT-4o, GPT-4o mini, GPT-4, GPT-4 Turbo with Vision, DALLE-3, Whisper, and Embeddings model series with the data residency, scalability, safety, security, and enterprise capabilities of Azure.
 
 If you have code that uses the OpenAI SDK, you can easily target your code to use the Azure OpenAI service. First, install the OpenAI SDK:
 
@@ -233,9 +233,9 @@ To learn more about using the Azure AI inferencing client, check out the [Azure
 
 ::: zone pivot="programming-language-python"
 
-## Prompt Templates
+## Prompt templates
 
-The inferencing client supports for creating prompt messages from templates.  The template allows you to dynamically generate prompts using inputs that are available at runtime.
+The inferencing client supports creating prompt messages from templates. The template allows you to dynamically generate prompts using inputs that are available at runtime.
 
 To use prompt templates, install the `azure-ai-inference` package:
 
@@ -356,7 +356,7 @@ To learn more about using Azure AI Search, check out [Azure AI Search documentat
 
 ## Azure AI Agent Service
 
-Azure AI Agent Service is a fully managed service designed to empower developers to securely build, deploy, and scale high-quality, and extensible AI agents. Using an extensive ecosystem of models, tools and capabilities from OpenAI, Microsoft, and third-party providers, [Azure AI Agent Service](/azure/ai-services/agents) enables building agents for a wide range of generative AI use cases.
+Azure AI Agent Service is a fully managed service designed to empower developers to securely build, deploy, and scale high-quality, and extensible AI agents. To enable building agents for a wide range of generative AI use cases, [Azure AI Agent Service](/azure/ai-services/agents) uses an extensive ecosystem of models, tools and capabilities from OpenAI, Microsoft, and third-party providers.
 
 ## Evaluation
 
@@ -393,7 +393,7 @@ To learn more, check out [Evaluation using the SDK](evaluate-sdk.md).
 
 ::: zone pivot="programming-language-csharp"
 
-An Azure AI evaluation package is not yet available for C#. For a sample on how to use Prompty and Semantic Kernel for evaluation, see the [contoso-chat-csharp-prompty](https://github.com/Azure-Samples/contoso-chat-csharp-prompty/blob/main/src/ContosoChatAPI/ContosoChat.Evaluation.Tests/Evalutate.cs) sample.
+An Azure AI evaluation package isn't yet available for C#. For a sample on how to use Prompty and Semantic Kernel for evaluation, see the [contoso-chat-csharp-prompty](https://github.com/Azure-Samples/contoso-chat-csharp-prompty/blob/main/src/ContosoChatAPI/ContosoChat.Evaluation.Tests/Evalutate.cs) sample.
 
 ::: zone-end
 
@@ -428,41 +428,43 @@ if application_insights_connection_string:
 
 ::: zone pivot="programming-language-csharp"
 
-Tracing is not yet integrated into the projects package. For instructions on how to instrument and log traces from the Azure AI Inferencing package, see [azure-sdk-for-dotnet](https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/ai/Azure.AI.Inference/samples/Sample8_ChatCompletionsWithOpenTelemetry.md).
+Tracing isn't yet integrated into the projects package. For instructions on how to instrument and log traces from the Azure AI Inferencing package, see [azure-sdk-for-dotnet](https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/ai/Azure.AI.Inference/samples/Sample8_ChatCompletionsWithOpenTelemetry.md).
 
 ::: zone-end
 
-## Related content
+## Other services and frameworks
 
-Below are some helpful links to other services and frameworks that you can use with the Azure AI Foundry SDK.
+The following sections provide helpful links to other services and frameworks that you can use with the Azure AI Foundry SDK.
 
 ### Azure AI Services
 
-Client libraries:
+* Client libraries:
 
-* [Azure AI services SDKs](../../../ai-services/reference/sdk-package-resources.md?context=/azure/ai-studio/context/context)
-* [Azure AI services REST APIs](../../../ai-services/reference/rest-api-resources.md?context=/azure/ai-studio/context/context) 
+    * [Azure AI services SDKs](../../../ai-services/reference/sdk-package-resources.md?context=/azure/ai-studio/context/context)
+    * [Azure AI services REST APIs](../../../ai-services/reference/rest-api-resources.md?context=/azure/ai-studio/context/context) 
 
-Management libraries:
-* [Azure AI Services Python Management Library](/python/api/overview/azure/mgmt-cognitiveservices-readme)
-* [Azure AI Search Python Management Library](/python/api/azure-mgmt-search/azure.mgmt.search)
+* Management libraries:
+
+    * [Azure AI Services Python Management Library](/python/api/overview/azure/mgmt-cognitiveservices-readme)
+    * [Azure AI Search Python Management Library](/python/api/azure-mgmt-search/azure.mgmt.search)
 
 ### Frameworks
 
-Azure Machine Learning
+* Azure Machine Learning
+
+    * [Azure Machine Learning Python SDK (v2)](/python/api/overview/azure/ai-ml-readme)
+    * [Azure Machine Learning CLI (v2)](/azure/machine-learning/how-to-configure-cli)
+    * [Azure Machine Learning REST API](/rest/api/azureml) 
 
-* [Azure Machine Learning Python SDK (v2)](/python/api/overview/azure/ai-ml-readme)
-* [Azure Machine Learning CLI (v2)](/azure/machine-learning/how-to-configure-cli)
-* [Azure Machine Learning REST API](/rest/api/azureml) 
+* Prompt flow
 
-Prompt flow
+    * [Prompt flow SDK](https://microsoft.github.io/promptflow/how-to-guides/quick-start.html)
+    * [pfazure CLI](https://microsoft.github.io/promptflow/reference/pfazure-command-reference.html)
+    * [pfazure Python library](https://microsoft.github.io/promptflow/reference/python-library-reference/promptflow-azure/promptflow.azure.html)
 
-* [Prompt flow SDK](https://microsoft.github.io/promptflow/how-to-guides/quick-start.html)
-* [pfazure CLI](https://microsoft.github.io/promptflow/reference/pfazure-command-reference.html)
-* [pfazure Python library](https://microsoft.github.io/promptflow/reference/python-library-reference/promptflow-azure/promptflow.azure.html)
+* Semantic Kernel
+    * [Semantic Kernel Overview](/semantic-kernel/overview/)
 
-Semantic Kernel
- * [Semantic Kernel Overview](/semantic-kernel/overview/)
-Agentic frameworks
+* Agentic frameworks
 
-* [LlamaIndex](llama-index.md)
+    * [LlamaIndex](llama-index.md)
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "SDK概要に関する情報の更新"
}
```

### Explanation
この変更では、「sdk-overview.md」ドキュメントが更新され、Azure AI Foundry SDKに関する情報が強化されました。主な変更点には、説明の明確化と最新の情報へのリンク追加が含まれています。

具体的には、以下の点が修正または追加されました：
- Azure AI Foundry SDKへのリンクが追加され、読者が直接アクセスできるようになりました。これにより、SDKの利用方法に関する情報がより明確になります。
- 日付が更新され、最新のリリース情報に置き換えられました。
- 文中のいくつかのフレーズが文法的に調整され、より自然な言い回しが採用されています。
- セクションのタイトルが変更され、各セクション内の構造が整理されました。

全体的に、この変更により、Azure AI Foundry SDKについての理解が向上し、開発者が利用する際に役立つ情報が提供されています。情報が最新化され、ユーザーがより効果的にSDKを活用できるようになっています。

## articles/ai-studio/how-to/develop/vscode.md{#item-24bd97}

<details>
<summary>Diff</summary>
````diff
@@ -20,7 +20,7 @@ author: sdgilley
 
 [!INCLUDE [feature-preview](../../includes/feature-preview.md)]
 
-Azure AI Foundry supports developing in VS Code - Desktop and Web. In each scenario, your VS Code instance is remotely connected to a prebuilt custom container running on a virtual machine, also known as a compute instance.
+[Azure AI Foundry](https://ai.azure.com) supports developing in VS Code - Desktop and Web. In each scenario, your VS Code instance is remotely connected to a prebuilt custom container running on a virtual machine, also known as a compute instance.
 
 ## Launch VS Code from Azure AI Foundry
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "VS Code関連の情報にリンクの追加"
}
```

### Explanation
この変更では、「vscode.md」ドキュメント内にAzure AI Foundryに関する重要なリンクが追加されました。具体的には、Azure AI Foundryを指す文に対して、公式サイトへのハイパーリンクが挿入されています。この変更により、読者はAzure AI Foundryについての詳細な情報に簡単にアクセスできるようになります。

修正内容は次の通りです：
- 「Azure AI Foundry supports developing in VS Code - Desktop and Web」という文が「[Azure AI Foundry](https://ai.azure.com) supports developing in VS Code - Desktop and Web」と書き換えられ、関連のウェブサイトへのリンクが設置されました。
- 変更により、ユーザーがAzure AI Foundryに関連するリソースや情報を簡単に探索できるようになり、開発環境の理解が促進されます。

この軽微な更新は、ドキュメントをよりインタラクティブにし、ユーザーエクスペリエンスを向上させる効果があります。

## articles/ai-studio/how-to/model-catalog-overview.md{#item-278001}

<details>
<summary>Diff</summary>
````diff
@@ -84,7 +84,7 @@ Gretel | Not available | Gretel-Navigator
 Healthcare AI family Models | MedImageParse<BR>  MedImageInsight<BR>  CxrReportGen<BR>  Virchow<BR>  Virchow2<BR>  Prism<BR>  BiomedCLIP-PubMedBERT<BR>  microsoft-llava-med-v1.5<BR>  m42-health-llama3-med4<BR>  biomistral-biomistral-7b<BR>  microsoft-biogpt-large-pub<BR>  microsoft-biomednlp-pub<BR>  stanford-crfm-biomedlm<BR>  medicalai-clinicalbert<BR>  microsoft-biogpt<BR>  microsoft-biogpt-large<BR>  microsoft-biomednlp-pub<BR> | Not Available
 JAIS | Not available | jais-30b-chat
 Meta Llama family models | Llama-3.3-70B-Instruct<BR> Llama-3.2-3B-Instruct<BR>  Llama-3.2-1B-Instruct<BR>  Llama-3.2-1B<BR>  Llama-3.2-90B-Vision-Instruct<BR>  Llama-3.2-11B-Vision-Instruct<BR>  Llama-3.1-8B-Instruct<BR>  Llama-3.1-8B<BR>  Llama-3.1-70B-Instruct<BR>  Llama-3.1-70B<BR>  Llama-3-8B-Instruct<BR>  Llama-3-70B<BR>  Llama-3-8B<BR>  Llama-Guard-3-1B<BR>  Llama-Guard-3-8B<BR>  Llama-Guard-3-11B-Vision<BR>  Llama-2-7b<BR>  Llama-2-70b<BR>  Llama-2-7b-chat<BR>  Llama-2-13b-chat<BR>  CodeLlama-7b-hf<BR>  CodeLlama-7b-Instruct-hf<BR>  CodeLlama-34b-hf<BR>  CodeLlama-34b-Python-hf<BR>  CodeLlama-34b-Instruct-hf<BR>  CodeLlama-13b-Instruct-hf<BR>  CodeLlama-13b-Python-hf<BR>  Prompt-Guard-86M<BR>  CodeLlama-70b-hf<BR> | Llama-3.3-70B-Instruct<BR> Llama-3.2-90B-Vision-Instruct<br>  Llama-3.2-11B-Vision-Instruct<br>  Llama-3.1-8B-Instruct<br>  Llama-3.1-70B-Instruct<br>  Llama-3.1-405B-Instruct<br>  Llama-3-8B-Instruct<br>  Llama-3-70B-Instruct<br>  Llama-2-7b<br>  Llama-2-7b-chat<br>  Llama-2-70b<br>  Llama-2-70b-chat<br>  Llama-2-13b<br>  Llama-2-13b-chat<br>
-Microsoft Phi family models | Phi-3-mini-4k-Instruct <br> Phi-3-mini-128k-Instruct <br> Phi-3-small-8k-Instruct <br> Phi-3-small-128k-Instruct <br> Phi-3-medium-4k-instruct <br> Phi-3-medium-128k-instruct <br> Phi-3-vision-128k-Instruct <br> Phi-3.5-mini-Instruct <br> Phi-3.5-vision-Instruct <br> Phi-3.5-MoE-Instruct <br> Phi-4 <br> Phi-4-mini-instruct | Phi-3-mini-4k-Instruct <br> Phi-3-mini-128k-Instruct <br> Phi-3-small-8k-Instruct <br> Phi-3-small-128k-Instruct <br> Phi-3-medium-4k-instruct <br> Phi-3-medium-128k-instruct <br> <br> Phi-3.5-mini-Instruct <br> Phi-3.5-vision-Instruct <br> Phi-3.5-MoE-Instruct <br> Phi-4 <br> Phi-4-mini-instruct
+Microsoft Phi family models | Phi-3-mini-4k-Instruct <br> Phi-3-mini-128k-Instruct <br> Phi-3-small-8k-Instruct <br> Phi-3-small-128k-Instruct <br> Phi-3-medium-4k-instruct <br> Phi-3-medium-128k-instruct <br> Phi-3-vision-128k-Instruct <br> Phi-3.5-mini-Instruct <br> Phi-3.5-vision-Instruct <br> Phi-3.5-MoE-Instruct <br> Phi-4 <br> Phi-4-mini-instruct <br> Phi-4-multimodal-instruct | Phi-3-mini-4k-Instruct <br> Phi-3-mini-128k-Instruct <br> Phi-3-small-8k-Instruct <br> Phi-3-small-128k-Instruct <br> Phi-3-medium-4k-instruct <br> Phi-3-medium-128k-instruct <br> <br> Phi-3.5-mini-Instruct <br> Phi-3.5-vision-Instruct <br> Phi-3.5-MoE-Instruct <br> Phi-4 <br> Phi-4-mini-instruct <br> Phi-4-multimodal-instruct
 Mistral family models | mistralai-Mixtral-8x22B-v0-1 <br> mistralai-Mixtral-8x22B-Instruct-v0-1 <br> mistral-community-Mixtral-8x22B-v0-1 <br> mistralai-Mixtral-8x7B-v01 <br> mistralai-Mistral-7B-Instruct-v0-2 <br> mistralai-Mistral-7B-v01 <br> mistralai-Mixtral-8x7B-Instruct-v01 <br> mistralai-Mistral-7B-Instruct-v01 | Mistral-large (2402) <br> Mistral-large (2407) <br> Mistral-small <br> Ministral-3B <br> Mistral-NeMo
 Nixtla | Not available | TimeGEN-1
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Microsoft Phiファミリーモデルの表に新しいエントリーの追加"
}
```

### Explanation
この変更は、「model-catalog-overview.md」ドキュメントにおけるMicrosoft Phiファミリーモデルの情報を更新したものです。具体的には、Phiファミリーのモデルリストに新たに「Phi-4-multimodal-instruct」が追加されました。

主な修正内容は以下の通りです：
- Microsoft Phiファミリーモデルのセクションに、「Phi-4-multimodal-instruct」という新しいモデルが追加され、より多様な用途に対応することを示しています。
- 変更に伴い、もともとあった表記に新たな情報が加わる形でリストが更新され、モデルの幅が広がっています。

この更新は、開発者や研究者が選択できるモデルの選択肢を増やし、ユーザーがAzure AIを利用する際に新しい機能や可能性を探る手助けとなります。

## articles/ai-studio/includes/region-availability-maas.md{#item-35d79c}

<details>
<summary>Diff</summary>
````diff
@@ -61,7 +61,7 @@ Llama 3.1 405B Instruct  | [Microsoft Managed countries/regions](/partner-center
 
 | Model | Offer Availability Region  | Hub/Project Region for Deployment  | Hub/Project Region for Fine tuning  |
 |---------|---------|---------|---------|
-Phi-4 <br>  Phi-4-mini-instruct    | Not applicable | East US <br> East US 2 <br> North Central US <br> South Central US <br> Sweden Central <br> West US <br> West US 3  | Not available       |
+Phi-4 <br>  Phi-4-mini-instruct <br>  Phi-4-multimodal-instruct    | Not applicable | East US <br> East US 2 <br> North Central US <br> South Central US <br> Sweden Central <br> West US <br> West US 3  | Not available       |
 Phi-3.5-vision-Instruct     | Not applicable | East US <br> East US 2 <br> North Central US <br> South Central US <br> Sweden Central <br> West US <br> West US 3  | Not available       |
 Phi-3.5-MoE-Instruct     | Not applicable | East US <br> East US 2 <br> North Central US <br> South Central US <br> Sweden Central <br> West US <br> West US 3  | East US 2       |
 Phi-3.5-Mini-Instruct     | Not applicable | East US <br> East US 2 <br> North Central US <br> South Central US <br> Sweden Central <br> West US <br> West US 3  | East US 2  | East US 2       |
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Phi-4ファミリーのモデルに新しい項目の追加"
}
```

### Explanation
この変更では、「region-availability-maas.md」ドキュメントにおいて、Phi-4ファミリーのモデル情報を更新しました。具体的には、「Phi-4-multimodal-instruct」という新しいモデルが追加されました。

主な修正内容は以下の通りです：
- Phi-4モデルのセクションに新たに「Phi-4-multimodal-instruct」が追加され、リストが更新されました。これにより、ユーザーはより多様な利用シナリオに対応するモデルを選択できるようになります。
- 他のモデルとのバランスを保ちつつ、新しいモデル情報が整然とした形で追加されているため、内容がより明確かつ包括的になっています。

この軽微な更新は、ドキュメントの情報を最新の状態に保つために重要であり、ユーザーや開発者がモデルの利用可能性を理解しやすくする助けとなります。

## articles/ai-studio/quickstarts/get-started-code.md{#item-8a5082}

<details>
<summary>Diff</summary>
````diff
@@ -16,7 +16,7 @@ author: sdgilley
 
 [!INCLUDE [feature-preview](../includes/feature-preview.md)]
 
-In this quickstart, we walk you through setting up your local development environment with the Azure AI Foundry SDK. We write a prompt, run it as part of your app code, trace the LLM calls being made, and run a basic evaluation on the outputs of the LLM.
+In this quickstart, we walk you through setting up your local development environment with the [Azure AI Foundry](https://ai.azure.com) SDK. We write a prompt, run it as part of your app code, trace the LLM calls being made, and run a basic evaluation on the outputs of the LLM.
 
 ## Prerequisites
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Azure AI Foundryリンクの追加"
}
```

### Explanation
この変更は、「get-started-code.md」ドキュメントにおいて、Azure AI Foundry SDKの紹介部分にリンクを追加したものです。具体的には、Azure AI Foundryの名前がハイパーリンクとして設定され、関連するリソースに直接アクセスできるようになっています。

主な修正内容は以下の通りです：
- 「Azure AI Foundry SDK」というテキストが、関連する公式サイトのリンクとして設定されました。これにより、読者はSDKに関する詳しい情報やリソースに容易にアクセスできるようになります。
- この変更は、ユーザーにとっての利便性を向上させ、クイックスタートガイドの内容をより直感的に理解できるようにします。

この軽微な更新は、情報の明確化と利用者の利便性を高めることを目的とした重要なステップです。

## articles/ai-studio/tutorials/copilot-sdk-build-rag.md{#item-b77dba}

<details>
<summary>Diff</summary>
````diff
@@ -15,7 +15,7 @@ ms.custom: copilot-learning-hub, ignite-2024
 
 # Tutorial:  Part 2 - Build a custom knowledge retrieval (RAG) app with the Azure AI Foundry SDK
 
-In this tutorial, you use the Azure AI Foundry SDK (and other libraries) to build, configure, and evaluate a chat app for your retail company called Contoso Trek. Your retail company specializes in outdoor camping gear and clothing. The chat app should answer questions about your products and services. For example, the chat app can answer questions such as "which tent is the most waterproof?" or "what is the best sleeping bag for cold weather?".
+In this tutorial, you use the [Azure AI Foundry](https://ai.azure.com) SDK (and other libraries) to build, configure, and evaluate a chat app for your retail company called Contoso Trek. Your retail company specializes in outdoor camping gear and clothing. The chat app should answer questions about your products and services. For example, the chat app can answer questions such as "which tent is the most waterproof?" or "what is the best sleeping bag for cold weather?".
 
 This part two shows you how to enhance a basic chat application by adding [retrieval augmented generation (RAG)](../concepts/retrieval-augmented-generation.md) to ground the responses in your custom data. Retrieval Augmented Generation (RAG) is a pattern that uses your data with a large language model (LLM) to generate answers specific to your data. In this part two, you learn how to:
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Azure AI Foundryリンクの追加"
}
```

### Explanation
この変更は、「copilot-sdk-build-rag.md」ドキュメントにおいて、Azure AI Foundry SDKに関する部分にリンクを追加しました。具体的には、Azure AI Foundryの名前がハイパーリンクとして設定され、関連するリソースに直接アクセスできるようになっています。

主な修正内容は以下の通りです：
- 「Azure AI Foundry SDK」というテキストがハイパーリンクとして追加され、利用者が公式サイトに簡単にアクセスできるようになりました。これにより、読者はSDKに関する更なる情報やリソースを迅速に見つけることができます。
- この変更は、チュートリアルの内容を補完し、読者が必要な情報に迅速にアクセスできるようにすることを意図しています。

この軽微な更新は、情報のアクセス性を向上させ、ユーザー体験をより良くするための重要なステップです。

## articles/ai-studio/tutorials/copilot-sdk-create-resources.md{#item-552960}

<details>
<summary>Diff</summary>
````diff
@@ -16,7 +16,7 @@ author: sdgilley
 
 # Tutorial:  Part 1 - Set up project and development environment to build a custom knowledge retrieval (RAG) app with the Azure AI Foundry SDK
 
-In this tutorial, you use the Azure AI Foundry SDK (and other libraries) to build, configure, and evaluate a chat app for your retail company called Contoso Trek. Your retail company specializes in outdoor camping gear and clothing. The chat app should answer questions about your products and services. For example, the chat app can answer questions such as "which tent is the most waterproof?" or "what is the best sleeping bag for cold weather?".
+In this tutorial, you use the [Azure AI Foundry](https://ai.azure.com) SDK (and other libraries) to build, configure, and evaluate a chat app for your retail company called Contoso Trek. Your retail company specializes in outdoor camping gear and clothing. The chat app should answer questions about your products and services. For example, the chat app can answer questions such as "which tent is the most waterproof?" or "what is the best sleeping bag for cold weather?".
 
 This tutorial is part one of a three-part tutorial.  This part one gets you ready to write code in part two and evaluate your chat app in part three. In this part, you:
 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Azure AI Foundryリンクの追加"
}
```

### Explanation
この変更は、「copilot-sdk-create-resources.md」ドキュメントにおいて、Azure AI Foundry SDKに関連する部分にリンクを追加しました。具体的には、Azure AI Foundryの名称がハイパーリンクとして設定され、読者が関連リソースに直接アクセスできるようになっています。

主な修正内容は以下の通りです：
- 「Azure AI Foundry SDK」というテキストがハイパーリンクとして追加され、利用者が公式サイトに簡単にアクセスできるようになっています。これにより、読者はSDKに関する詳細情報やリソースを迅速に見つけることができます。
- この変更は、チュートリアルの内容をより充実させ、ユーザーが必要な情報に簡単にアクセスできるようにすることを目的としています。

この軽微な更新は、情報の明確化とユーザー体験の向上を図った重要な改善です。

## articles/ai-studio/tutorials/copilot-sdk-evaluate.md{#item-bb5754}

<details>
<summary>Diff</summary>
````diff
@@ -16,7 +16,7 @@ author: sdgilley
 
 # Tutorial: Part 3 - Evaluate a custom chat application with the Azure AI Foundry SDK
 
-In this tutorial, you use the Azure AI SDK (and other libraries) to  evaluate the chat app you built in [Part 2 of the tutorial series](copilot-sdk-build-rag.md). In this part three, you learn how to:
+In this tutorial, you use the [Azure AI Foundry](https://ai.azure.com) SDK (and other libraries) to  evaluate the chat app you built in [Part 2 of the tutorial series](copilot-sdk-build-rag.md). In this part three, you learn how to:
 
 > [!div class="checklist"]
 > - Create an evaluation dataset
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Azure AI Foundryリンクの追加"
}
```

### Explanation
この変更は、「copilot-sdk-evaluate.md」ドキュメントにおいて、Azure AI Foundry SDKに関する部分にリンクを追加しました。具体的には、「Azure AI SDK」というテキストがハイパーリンクとして設定され、読者が関連リソースに直接アクセスできるようになっています。

主な修正内容は以下の通りです：
- 「Azure AI SDK」という名称が「[Azure AI Foundry](https://ai.azure.com) SDK」として修正され、ハイパーリンクが提供されています。これにより、利用者はSDKに関する詳細な情報やリソースを迅速に見つけることができます。
- この変更は、チュートリアルの内容をより充実させ、ユーザーが必要な情報に簡単にアクセスできるようにすることを目的としています。

この軽微な更新は、情報の明確さを向上させ、ユーザー体験をより良くするための重要な改善です。

## articles/ai-studio/tutorials/screen-reader.md{#item-4dc029}

<details>
<summary>Diff</summary>
````diff
@@ -20,7 +20,7 @@ This article is for people who use screen readers such as [Microsoft's Narrator]
 
 ## Getting oriented in Azure AI Foundry portal 
 
-Most Azure AI Foundry pages are composed of the following landmark structure: 
+Most [Azure AI Foundry](https://ai.azure.com) pages are composed of the following landmark structure: 
 
 - Banner (contains Azure AI Foundry app title, settings, and profile information)
     - Might sometimes contain a breadcrumb navigation element 
````
</details>

### Summary

```json
{
    "modification_type": "minor update",
    "modification_title": "Azure AI Foundryリンクの追加"
}
```

### Explanation
この変更は、「screen-reader.md」ドキュメントにおいて、Azure AI Foundryに関連する部分にリンクを追加しました。具体的には、Azure AI Foundryの名称がハイパーリンクとして設定され、読者が関連情報に直接アクセスできるようになっています。

主な修正内容は以下の通りです：
- 「Azure AI Foundry」というテキストが「[Azure AI Foundry](https://ai.azure.com)」としてハイパーリンクに変更されました。これにより、利用者はAzure AI Foundryのポータルに簡単にアクセスし、必要な情報を迅速に得ることができます。
- この変更は、画面リーダーを使用するユーザーに対して、リソースアクセスを簡素化することを目的としています。

この軽微な更新は、ドキュメントの情報の明確さを向上させ、ユーザーが必要な資源にアクセスしやすくなるために重要です。


